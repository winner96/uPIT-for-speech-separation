{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uPIT_f.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winner96/uPIT-for-speech-separation/blob/master/uPIT_f.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvagQwpl5hS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc234f31-f780-4288-f557-852c1eee0bd7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3HrNpq95y6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26a2580-ec99-40f6-f822-b384bbf2f4b9"
      },
      "source": [
        "!git clone https://github.com/winner96/uPIT-for-speech-separation.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'uPIT-for-speech-separation'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 118 (delta 9), reused 0 (delta 0), pack-reused 90\u001b[K\n",
            "Receiving objects: 100% (118/118), 42.13 KiB | 8.42 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIzl2Zd_guse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd8ff3f-6f2e-4cf3-bfb6-62816cfbca95"
      },
      "source": [
        "root = \"/content/uPIT-for-speech-separation\"\n",
        "!echo $root"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/uPIT-for-speech-separation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m9VY2m28cKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9b3916-a96f-45db-e7da-36561136b9b3"
      },
      "source": [
        "%cd $root\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/uPIT-for-speech-separation\n",
            "compute_cmvn.py  dataset.py   oracle_separate.py  run_pit.py   trainer.py\n",
            "conf\t\t make_scp.py  README.md\t\t  scripts      uPIT_f.ipynb\n",
            "configs\t\t model.py     requirements.txt\t  separate.py  utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x23NN_CLPRr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3807dbf0-29fd-4660-ea15-0af8ba154f66"
      },
      "source": [
        "avs_data_z = \"avs_audio.zip\"\n",
        "avs_data = \"avs_audio\"\n",
        "\n",
        "print(\"start donwload\")\n",
        "![ ! -f $avs_data_z ] && rsync /content/drive/MyDrive/$avs_data_z ./$avs_data_z && echo \"zip downloaded\"\n",
        "\n",
        "print(\"start unzip\")\n",
        "\n",
        "![ ! -d $root/$avs_data ] && jar xf ./$avs_data_z && echo \"zip unziped\"\n",
        "![ -d $root/$avs_data ] && rm ./$avs_data_z && echo \"remove .zip\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start donwload\n",
            "zip downloaded\n",
            "start unzip\n",
            "zip unziped\n",
            "remove .zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1G1q0Lg6Iaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16a094ed-4222-4baf-fa4a-46d7e01a2e67"
      },
      "source": [
        "pip install -r $root/requirements.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/50a05de5337f7a924bb8bd70c6936230642233e424d6a9747ef1cfbde353/torch-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (773.1MB)\n",
            "\u001b[K     |████████████████████████████████| 773.1MB 21kB/s \n",
            "\u001b[?25hCollecting tqdm==4.32.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/af/685bf3ce889ea191f3b916557f5677cc95a5e87b2fa120d74b5dd6d049d0/tqdm-4.32.1-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hCollecting librosa==0.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/71/c21ccef81276d85b9e3a36d80dad5baaf8a91f912e65eff0fd0d74a5a19c/librosa-0.7.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 43.6MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2MB 132kB/s \n",
            "\u001b[?25hCollecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 117kB/s \n",
            "\u001b[?25hCollecting PyYAML==5.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 54.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (0.17.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (0.48.0)\n",
            "Collecting soundfile>=0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (50.3.2)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (0.31.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.9.0->librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (1.14.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.1->-r /content/uPIT-for-speech-separation/requirements.txt (line 3)) (2.20)\n",
            "Building wheels for collected packages: librosa, PyYAML\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.1-cp36-none-any.whl size=1610157 sha256=e8a7fd64f844f72f8e0a021f75b7295f08aa2b2abfe57817104f1c53daf47b43\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/36/47/a9a4d151332cbdaec564500af9704a0ad862cf554dcf4bfda0\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.1.1-cp36-cp36m-linux_x86_64.whl size=44100 sha256=1f6b8318e190f14365af08fa8df30edbd1a6c6f12e01515175fd3e1a2bce9014\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
            "Successfully built librosa PyYAML\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.32.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.32.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, torch, tqdm, scipy, soundfile, librosa, PyYAML\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: librosa 0.6.3\n",
            "    Uninstalling librosa-0.6.3:\n",
            "      Successfully uninstalled librosa-0.6.3\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.1.1 librosa-0.7.1 numpy-1.16.4 scipy-1.3.0 soundfile-0.10.3.post1 torch-1.3.0 tqdm-4.32.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "905WqrmP9rXe"
      },
      "source": [
        "!chmod -R 755 $root"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwm8K6M5Xo_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fba09d6-714e-4a1f-d0b7-5ff6c5695e47"
      },
      "source": [
        "!python $root/make_scp.py"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/uPIT-for-speech-separation/scp already exists\n",
            "train scp done\n",
            "test scp done\n",
            "cross validation scp done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwpG6BDk7ot9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c0a002-6631-4ffa-cc3e-2fb57175c79a"
      },
      "source": [
        "![ ! -f $root/cmvn.dict ] && python $root/compute_cmvn.py $root/scp/tr_mx.scp $root/cmvn.dict"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-23 01:04:07,968 [/content/uPIT-for-speech-separation/dataset.py:32 - INFO ] Create SpectrogramReader for /content/uPIT-for-speech-separation/scp/tr_mx.scp with 11000 utterances\n",
            "100% 11000/11000 [01:15<00:00, 145.18it/s]\n",
            "Totally processed 10247395 frames\n",
            "Global mean: [0.2997594  0.71926643 1.63566245 2.32738478 2.49458489 2.35660675\n",
            " 2.38693876 2.42109148 2.24258472 1.96341174 1.69273107 1.42693255\n",
            " 1.17887962 0.96797977 0.81100943 0.70222181 0.62321968 0.56603316\n",
            " 0.52445333 0.49257595 0.46846244 0.44966679 0.43404743 0.42025397\n",
            " 0.40728411 0.39173565 0.37293576 0.35298567 0.33236566 0.31227881\n",
            " 0.29287571 0.27487152 0.25974601 0.24747444 0.23854263 0.23328922\n",
            " 0.23095619 0.2308874  0.231603   0.2315114  0.22964026 0.22535518\n",
            " 0.2188982  0.20928901 0.19855478 0.18839961 0.17883314 0.1703579\n",
            " 0.16347256 0.157687   0.15310585 0.14951231 0.14664498 0.1450826\n",
            " 0.14407301 0.14288463 0.1414998  0.13949926 0.13692807 0.13387796\n",
            " 0.13022445 0.12623853 0.12242838 0.11843276 0.11441298 0.11054469\n",
            " 0.10679325 0.10333015 0.10006186 0.09685012 0.09398808 0.09106975\n",
            " 0.08832944 0.08595224 0.08374079 0.08158243 0.07967177 0.07806372\n",
            " 0.0766778  0.07555947 0.07461382 0.07350431 0.07236911 0.07164109\n",
            " 0.07101046 0.07031794 0.0697438  0.069305   0.0686965  0.06824996\n",
            " 0.06784667 0.0674586  0.06716064 0.06681486 0.06651052 0.06626678\n",
            " 0.06596958 0.06559521 0.06508042 0.06469523 0.06443415 0.06408722\n",
            " 0.06351674 0.06306921 0.06252625 0.06205067 0.06164005 0.06099199\n",
            " 0.06017215 0.05939131 0.05857223 0.05759146 0.05647279 0.05514859\n",
            " 0.05353209 0.05169597 0.0497218  0.04760683 0.04528199 0.04271745\n",
            " 0.03985811 0.03686289 0.03386063 0.03083801 0.0278941  0.02523565\n",
            " 0.02321944 0.02222673 0.01992016]\n",
            "Global std: [0.85291226 1.27006832 2.45631737 3.16266591 3.47899845 3.51019059\n",
            " 3.70924323 3.8623524  3.71599318 3.3999045  3.11356712 2.8014308\n",
            " 2.45115559 2.08278077 1.79443966 1.58188846 1.41878915 1.30310984\n",
            " 1.21546925 1.14784185 1.09865544 1.054837   1.01210833 0.97223407\n",
            " 0.9444232  0.91327575 0.87164791 0.82607518 0.77980452 0.7243565\n",
            " 0.67428433 0.62706667 0.58368391 0.55010046 0.52264354 0.50326244\n",
            " 0.48960985 0.48575355 0.48410288 0.47987004 0.47665432 0.47357384\n",
            " 0.46894541 0.45115763 0.42981983 0.41506369 0.39873831 0.38047587\n",
            " 0.37131922 0.35871518 0.34666424 0.33723098 0.32990596 0.3290215\n",
            " 0.33061799 0.3303462  0.32807025 0.3241183  0.31896455 0.31334706\n",
            " 0.30310989 0.29337655 0.28729624 0.27888686 0.27097968 0.26484061\n",
            " 0.2568525  0.2513442  0.24752823 0.24296941 0.24001011 0.23484755\n",
            " 0.22939394 0.22585764 0.22295257 0.21998641 0.21707164 0.21608957\n",
            " 0.21475708 0.21374931 0.21317624 0.2098803  0.20768521 0.2073951\n",
            " 0.20691735 0.20520247 0.20447041 0.20505629 0.20370077 0.20334117\n",
            " 0.20327913 0.2040705  0.20545828 0.20473012 0.20381725 0.20418845\n",
            " 0.20364188 0.20208186 0.19939063 0.19836464 0.19904054 0.19924191\n",
            " 0.19655335 0.19512472 0.19466751 0.19287041 0.19222854 0.19113142\n",
            " 0.18841645 0.18578395 0.18414391 0.1816613  0.17859624 0.17543125\n",
            " 0.1708741  0.16631411 0.16099259 0.1551363  0.14858665 0.14098687\n",
            " 0.13140164 0.1219833  0.11273819 0.10353485 0.09266337 0.08353036\n",
            " 0.07661715 0.0730213  0.07366484]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3imLtUEbXHy"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmeaP62NBtJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf6cb74-aa65-4d6d-c9d0-4e2f2b3c2856"
      },
      "source": [
        "#!python3 /content/uPIT-for-speech-separation/run_pit.py --config /content/drive/MyDrive/configs/2config.yaml --num-epoches 100 > /content/uPIT-for-speech-separation/check/train.log 2>&1 &\n",
        "\n",
        "\n",
        "!python $root/run_pit.py --config $root/configs/4fconfig.yaml --num-epoches 20\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-23 01:05:25,202 [/content/uPIT-for-speech-separation/run_pit.py:36 - INFO ] Start training in normal model\n",
            "/content/uPIT-for-speech-separation/utils.py:138: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  config_dict = yaml.load(f)\n",
            "2020-11-23 01:05:25,208 [/content/uPIT-for-speech-separation/run_pit.py:43 - INFO ] Training with PSM\n",
            "2020-11-23 01:05:25,208 [/content/uPIT-for-speech-separation/run_pit.py:47 - INFO ] Training in 8 utterance per batch\n",
            "2020-11-23 01:05:25,217 [/content/uPIT-for-speech-separation/dataset.py:32 - INFO ] Create SpectrogramReader for /content/uPIT-for-speech-separation/scp/tr_mx.scp with 11000 utterances\n",
            "2020-11-23 01:05:25,226 [/content/uPIT-for-speech-separation/dataset.py:32 - INFO ] Create SpectrogramReader for /content/uPIT-for-speech-separation/scp/tr_s1.scp with 11000 utterances\n",
            "2020-11-23 01:05:25,234 [/content/uPIT-for-speech-separation/dataset.py:32 - INFO ] Create SpectrogramReader for /content/uPIT-for-speech-separation/scp/tr_s2.scp with 11000 utterances\n",
            "2020-11-23 01:05:25,234 [/content/uPIT-for-speech-separation/dataset.py:171 - INFO ] Using cmvn dictionary from /content/uPIT-for-speech-separation/cmvn.dict\n",
            "2020-11-23 01:05:25,236 [/content/uPIT-for-speech-separation/dataset.py:32 - INFO ] Create SpectrogramReader for /content/uPIT-for-speech-separation/scp/cv_mx.scp with 1000 utterances\n",
            "2020-11-23 01:05:25,237 [/content/uPIT-for-speech-separation/dataset.py:32 - INFO ] Create SpectrogramReader for /content/uPIT-for-speech-separation/scp/cv_s1.scp with 1000 utterances\n",
            "2020-11-23 01:05:25,237 [/content/uPIT-for-speech-separation/dataset.py:32 - INFO ] Create SpectrogramReader for /content/uPIT-for-speech-separation/scp/cv_s2.scp with 1000 utterances\n",
            "2020-11-23 01:05:25,238 [/content/uPIT-for-speech-separation/dataset.py:171 - INFO ] Using cmvn dictionary from /content/uPIT-for-speech-separation/cmvn.dict\n",
            "2020-11-23 01:05:25,238 [/content/uPIT-for-speech-separation/run_pit.py:64 - INFO ] Training for 20 epoches -> /content/drive/MyDrive/checkpoint/2spk_pit_psm_b...\n",
            "2020-11-23 01:05:25,692 [/content/uPIT-for-speech-separation/trainer.py:61 - INFO ] Network structure:\n",
            "PITNet(\n",
            "  (rnn): LSTM(129, 896, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (drops): Dropout(p=0.5, inplace=False)\n",
            "  (linear): ModuleList(\n",
            "    (0): Linear(in_features=1792, out_features=129, bias=True)\n",
            "    (1): Linear(in_features=1792, out_features=129, bias=True)\n",
            "  )\n",
            ")\n",
            "2020-11-23 01:05:25,692 [/content/uPIT-for-speech-separation/trainer.py:34 - INFO ] Create optimizer adam: {'lr': 0.001, 'weight_decay': 0}\n",
            "2020-11-23 01:05:32,956 [/content/uPIT-for-speech-separation/trainer.py:83 - INFO ] Clip gradient by 2-norm 200\n",
            "2020-11-23 01:05:32,966 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 01:07:23,734 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 01:07:23,735 [/content/uPIT-for-speech-separation/trainer.py:134 - INFO ] Epoch  0: dev = 101.1842\n",
            "2020-11-23 01:07:27,600 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 01:15:28,781 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 01:23:39,573 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 01:31:54,845 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 01:40:11,711 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 01:48:23,111 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 01:52:29,837 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 01:52:29,839 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 01:54:31,767 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 01:54:31,768 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch  1: train = 43.4540(2702.24s/1375) | dev = 44.4011(121.93s/125)\n",
            "2020-11-23 01:54:33,493 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 02:02:34,027 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 02:10:44,029 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 02:18:53,890 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 02:27:04,473 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 02:35:13,921 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 02:39:21,035 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 02:39:21,037 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 02:41:20,130 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 02:41:20,131 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch  2: train = 40.3701(2687.54s/1375) | dev = 35.7286(119.09s/125)\n",
            "2020-11-23 02:41:21,440 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 02:49:18,328 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 02:57:22,335 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 03:05:28,645 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 03:13:37,717 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 03:21:41,892 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 03:25:48,716 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 03:25:48,717 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 03:27:48,264 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 03:27:48,265 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch  3: train = 34.2435(2667.28s/1375) | dev = 27.0335(119.55s/125)\n",
            "2020-11-23 03:27:49,088 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 03:35:43,426 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 03:43:48,505 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 03:51:56,123 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 04:00:02,683 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 04:08:06,513 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 04:12:11,552 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 04:12:11,553 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 04:14:10,597 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 04:14:10,598 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch  4: train = 26.1290(2662.46s/1375) | dev = 26.6331(119.05s/125)\n",
            "2020-11-23 04:14:11,396 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 04:22:09,451 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 04:30:16,956 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 04:38:25,132 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 04:46:28,442 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 04:54:34,925 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 04:58:39,132 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 04:58:39,134 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 05:00:38,977 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 05:00:38,978 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch  5: train = 25.8672(2667.74s/1375) | dev = 26.2321(119.84s/125)\n",
            "2020-11-23 05:00:39,805 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 05:08:44,924 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 05:16:48,570 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 05:24:55,073 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 05:33:01,849 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 05:41:10,538 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 05:45:17,640 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 05:45:17,642 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 05:47:17,681 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 05:47:17,682 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch  6: train = 25.7185(2677.84s/1375) | dev = 26.2632(120.04s/125)\n",
            "2020-11-23 05:47:18,516 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 05:55:15,875 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 06:03:20,697 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 06:11:30,181 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 06:19:35,939 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 06:27:43,622 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 06:31:49,975 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 06:31:49,977 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 06:33:50,334 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 06:33:50,335 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch  7: train = 25.5160(2671.46s/1375) | dev = 25.9511(120.36s/125)\n",
            "2020-11-23 06:33:51,284 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 06:41:48,039 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 06:49:56,614 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 06:58:05,967 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 07:06:10,501 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 07:14:14,224 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 07:18:14,796 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 07:18:14,797 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 07:20:13,504 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 07:20:13,505 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch  8: train = 25.4414(2663.51s/1375) | dev = 25.9157(118.71s/125)\n",
            "2020-11-23 07:20:14,408 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 07:28:08,441 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 07:36:14,828 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 07:44:20,289 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 07:52:20,466 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 08:00:20,243 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 08:04:26,420 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 08:04:26,422 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 08:06:25,766 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 08:06:25,767 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch  9: train = 25.2209(2652.01s/1375) | dev = 26.8061(119.34s/125)\n",
            "2020-11-23 08:06:26,608 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 08:14:19,841 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 08:22:19,223 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 08:30:26,382 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 08:38:33,536 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 08:46:38,339 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 08:50:40,914 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 08:50:40,915 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 08:52:39,677 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "Epoch     9: reducing learning rate of group 0 to 7.0000e-04.\n",
            "2020-11-23 08:52:39,678 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch 10: train = 25.1993(2654.31s/1375) | dev = 26.0816(118.76s/125)\n",
            "2020-11-23 08:52:40,546 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 09:00:34,221 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 09:08:37,229 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 09:16:40,143 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 09:24:41,610 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 09:32:44,854 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 09:36:46,718 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 09:36:46,719 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 09:38:44,646 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "2020-11-23 09:38:44,647 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch 11: train = 24.8352(2646.17s/1375) | dev = 26.0840(117.93s/125)\n",
            "2020-11-23 09:38:45,452 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 09:46:38,804 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 09:54:39,466 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 10:02:45,211 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n",
            "2020-11-23 10:10:51,695 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1000 batches, 8000 utterances\n",
            "2020-11-23 10:18:54,423 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 1250 batches, 10000 utterances\n",
            "2020-11-23 10:22:59,898 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 11000 utterances in total\n",
            "2020-11-23 10:22:59,898 [/content/uPIT-for-speech-separation/trainer.py:117 - INFO ] Cross Validate...\n",
            "2020-11-23 10:24:59,093 [/content/uPIT-for-speech-separation/dataset.py:307 - INFO ] Processed 1000 utterances in total\n",
            "Epoch    11: reducing learning rate of group 0 to 4.9000e-04.\n",
            "2020-11-23 10:24:59,095 [/content/uPIT-for-speech-separation/trainer.py:150 - INFO ] Loss(time/mini-batch) - Epoch 12: train = 24.5814(2654.45s/1375) | dev = 29.2478(119.20s/125)\n",
            "2020-11-23 10:25:00,044 [/content/uPIT-for-speech-separation/trainer.py:89 - INFO ] Training...\n",
            "2020-11-23 10:32:55,243 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 250 batches, 2000 utterances\n",
            "2020-11-23 10:40:55,637 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 500 batches, 4000 utterances\n",
            "2020-11-23 10:49:01,361 [/content/uPIT-for-speech-separation/dataset.py:305 - INFO ] Processed 750 batches, 6000 utterances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP0Z_Y4u3nuK"
      },
      "source": [
        "#code for separate\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}